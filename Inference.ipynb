{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a51ad2-32e9-4d13-86cc-0ccb84c54428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import isodisreg \n",
    "from isodisreg import idr\n",
    "import properscoring as ps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models\n",
    "import netCDF4 as nc4\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from scipy.ndimage import zoom\n",
    "from datetime import datetime\n",
    "import utils as ut\n",
    "import dependency as dep\n",
    "import models64 as models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "GPM = '/projectnb/labci/NAS/Data/GPM/Version6/Ghana/pr_20000602S_20210930E/'\n",
    "TIG = '/projectnb/labci/NAS/Project/ShortTermRainfallForecast_Ghana/version1/code/Inference/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9cba94-b5d9-4e23-b13b-79a39baca5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(time_step, cbar_label='Data Value', save=None):\n",
    "    num_models = len(models)\n",
    "    fig, axes = plt.subplots(1, num_models, figsize=(14 * num_models / 2, 8), constrained_layout=False, facecolor='white')\n",
    "    if num_models == 1:\n",
    "        axes = [axes]\n",
    "    fig.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    global_min = min(np.min(data[time_step, :, :]) for data in models.values())\n",
    "    global_max = max(np.max(data[time_step, :, :]) for data in models.values())\n",
    "\n",
    "    for i, (ax, (title, data)) in enumerate(zip(axes, models.items())):\n",
    "        ax.set_facecolor('white')\n",
    "        m = Basemap(projection='cyl', llcrnrlat=latitude.min(), urcrnrlat=latitude.max(),\n",
    "                    llcrnrlon=longitude.min(), urcrnrlon=longitude.max(), resolution='l', ax=ax)\n",
    "\n",
    "        m.drawcoastlines(linewidth=1.5, color='darkred')\n",
    "        m.drawcountries(linewidth=1.5, color='darkred')\n",
    "        m.drawparallels(np.arange(int(latitude.min()), int(latitude.max()) + 1, 1), labels=[1, 0, 0, 0], fontsize=20)\n",
    "        m.drawmeridians(np.arange(int(longitude.min()), int(longitude.max()) + 1, 1), labels=[0, 0, 0, 1], fontsize=20)\n",
    "\n",
    "        x, y = m(longitude, latitude)\n",
    "        cs = m.pcolormesh(x, y, data[time_step, :, :], cmap='Blues', shading='auto', vmin=global_min, vmax=global_max)\n",
    "        if '_' in title:\n",
    "            base_name, suffix = title.rsplit('_', 1)  # Split title into base name and suffix\n",
    "            ax.set_title(f\"{base_name}$_{{{suffix}}}$\", fontsize=24)\n",
    "        else:\n",
    "            ax.set_title(f\"{title}\", fontsize=24)\n",
    "\n",
    "\n",
    "        # Add colorbar only to the rightmost plot\n",
    "        if i == num_models - 1:\n",
    "            cbar = m.colorbar(cs, location='right', pad=\"5%\")\n",
    "            cbar.set_label(cbar_label, fontsize=24)\n",
    "            #cbar.set_ticks([-1, 0, 1])\n",
    "            #cbar.ax.set_yticklabels(['-1', '0', '1'], fontsize=20)\n",
    "            cbar.ax.tick_params(labelsize=20)\n",
    "         # Save only the first image\n",
    "        if save:\n",
    "            plt.savefig(save, bbox_inches='tight', dpi=600)\n",
    "            print(f\"Saved first image as {save}\")\n",
    "    plt.show()\n",
    "with open('/projectnb/labci/NAS/Project/ShortTermRainfallForecast_Ghana/version1/saved_data/lonlat_GPM_ERA.pkl', 'rb') as f:\n",
    "    GPMLong, GPMLat, _,_ = pickle.load(f)\n",
    "new_lat = np.linspace(GPMLat.max(), GPMLat.min(), 64)\n",
    "new_long = np.linspace(GPMLong.min(), GPMLong.max(), 64)\n",
    "latitude, longitude = np.array(new_lat), np.array(new_long)\n",
    "longitude, latitude = np.meshgrid(longitude, latitude)  # Create 2D grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880697fe-a6b3-446d-a3a3-86b0173b6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training data for CRPS\n",
    "#GPM original dimension=(54, 73) and ERA original dimension=(30, 23). Change the dimension of one dataset\n",
    "with open('/projectnb/labci/NAS/Project/ShortTermRainfallForecast_Ghana/version1/saved_data/date_all.pkl', 'rb') as f:\n",
    "    date = pickle.load(f)\n",
    "with open('/projectnb/labci/NAS/Project/ShortTermRainfallForecast_Ghana/version1/saved_data/model3/ERA5_64_64.pkl', 'rb') as f:\n",
    "    ERA = pickle.load(f)\n",
    "with open('/projectnb/labci/NAS/Project/ShortTermRainfallForecast_Ghana/version1/saved_data/model3/GPM_64_64.pkl', 'rb') as f:\n",
    "    GPM = pickle.load(f)\n",
    "    #GPM = np.clip(GPM, 0, None)\n",
    "print(GPM.shape, ERA.shape, np.min(GPM), np.max(GPM), date.shape, date)\n",
    "#Normalization\n",
    "Ndata = []\n",
    "for i in range(len(ERA)):\n",
    "    if(i<59):\n",
    "        Ndata.append(ut.znorm(ERA[i]))\n",
    "    else:\n",
    "        print('No normalization for as they are seasonal and time variables',i)\n",
    "        Ndata.append([ERA[i],0,0])\n",
    "ENorm = [item[0] for item in Ndata]\n",
    "Emean = [item[1] for item in Ndata] \n",
    "Estd = [item[2] for item in Ndata]\n",
    "GNorm, Gmean, Gstd  = ut.norm(GPM)\n",
    "def Preparedata(data,dataL,s,d,var, date):\n",
    "    X,Y,Z = [],[],[]\n",
    "    for i in range(0,len(data[0])-max(s,d)):\n",
    "        T, L = [], []\n",
    "        for j in var:\n",
    "            x = data[j][i:i+s]\n",
    "            T.append(x)\n",
    "        X.append(T),Y.append(dataL[i+s:i+s+d]),Z.append(int(date[i+s:i+s+d]))#Y.append(data[0][i+s])\n",
    "    return np.array(X), np.array(Y), np.array(Z)\n",
    "\n",
    "sequence, days, variable = 1, 1, [num for num in range(61) if num < 2 or num > 5]\n",
    "X,Y,Z = Preparedata(ENorm, GNorm, sequence, days, variable, date) #time sequence=3, can be 1 to 10; can send Ndata Ndata for normalize input and output\n",
    "if(len(variable)>1):\n",
    "    X = X.reshape(X.shape[0], X.shape[1]*X.shape[2], X.shape[3], X.shape[4])\n",
    "    #Y = np.expand_dims(Y,axis=1) # 0 represents the precipition data as output\n",
    "X = X.squeeze()\n",
    "###################Important#############################################\n",
    "num_images = len(X)\n",
    "split_ratio = [0.98, 0.01, 0.01]\n",
    "num_train = int(num_images * split_ratio[0])\n",
    "num_val = int(num_images * split_ratio[1])\n",
    "num_test = num_images - num_train - num_val\n",
    "if os.path.exists('/projectnb/labci/NAS/Project/ShortTermRainfallForecast_Ghana/version1/saved_data/permutation_indices.npy'):\n",
    "    print(\"Permutation file exists and loading it...\")\n",
    "    permutation_indices = np.load('/projectnb/labci/NAS/Project/ShortTermRainfallForecast_Ghana/version1/saved_data/permutation_indices.npy')\n",
    "else:\n",
    "    print(\"Permutation file does not exist and preparing it for future reference\")\n",
    "    permutation_indices = np.random.permutation(num_images)\n",
    "    #np.save('/projectnb/labci/Indrajit/Rainfall/data/ERA5/Version3/data/permutation_indices.npy', permutation_indices)\n",
    "X,Y,Z = X[permutation_indices],Y[permutation_indices],Z[permutation_indices]\n",
    "\n",
    "trainX, valX, testX = np.split(X, [num_train, num_train + num_val])\n",
    "trainY, valY, testY = np.split(Y, [num_train, num_train + num_val])\n",
    "trainZ, valZ, testZ = np.split(Z, [num_train, num_train + num_val])\n",
    "GPM_train = ut.CustomDataset(trainX, trainY)\n",
    "GPM_val = ut.CustomDataset(valX, valY)\n",
    "GPM_test = ut.CustomDataset(testX, testY)\n",
    "print(len(GPM_train), len(GPM_val), len(GPM_test))\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(GPM_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e0a63-01ce-4f61-8fce-edd37147c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rainy_pixels = GPM>=0\n",
    "percent_rainy_pixels = np.sum(rainy_pixels) / GPM.size * 100\n",
    "# Percentage of rainy days (any pixel > threshold)\n",
    "rainy_days = np.any(rainy_pixels, axis=(1, 2))\n",
    "percent_rainy_days = np.sum(rainy_days) / GPM.shape[0] * 100\n",
    "\n",
    "print(f\"Percentage of rainy pixels (all days): {percent_rainy_pixels:.2f}%\")\n",
    "print(f\"Percentage of rainy days (any pixel): {percent_rainy_days:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2dbe8-7aba-46a8-a0dd-43a0c26d63f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First do the evaluation for TIGG dataset\n",
    "GPM = '<GPM dataset path>'\n",
    "TIG = '<ECMWF Ensemble path>'\n",
    "GroundTruth, TIGG_Pturb, TIGG_Control, Date_Position, TargetDate = dep.Evaluation(GPM, TIG, GPM_zoom=True, GPM_desired_shape = (64, 64),\n",
    "                                                                              TIG_zoom=True, TIG_desired_shape = (64, 64))\n",
    "\n",
    "TIGG_Pred = np.mean(np.stack((TIGG_Pturb, TIGG_Control), axis=0), axis=0)\n",
    "print('TIGG prediction shape', TIGG_Pred.shape)\n",
    "Ind_TIG = np.array(dep.Indv_TIGG(TIG, TIG_zoom=True, TIG_desired_shape = (64, 64)))\n",
    "Ind_TIG_ens = np.concatenate((Ind_TIG, np.expand_dims(TIGG_Control, axis=1)), axis=1)# Combine the Ensemble of TIGG (50+1)\n",
    "print('TIGG shape for ensemble and CRPS', Ind_TIG_ens.shape)\n",
    "Random_Prediction, Ind_Random, Random_GT = dep.RandomEvaluation(GPM, TargetDate, GPM_zoom=True, GPM_desired_shape = (64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bf07ad-63ae-4e5e-a527-329b0719080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For model. First load the model and the test_loader\n",
    "model, test_loader, Gmean, Gstd, norm = dep.load_model()\n",
    "model1, test_loader1, _, _, _ = dep.load_model()\n",
    "model2, test_loader2, _, _, _ = dep.load_model()\n",
    "model3, test_loader3, _, _, _ = dep.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c7d70-9e93-45df-9dcc-0d10eca21f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model. First select the date that is available in TIGG and then create the loader and evaluate\n",
    "subset_inputs, subset_targets = dep.extract_ordered_subset_from_loader(test_loader, Date_Position)\n",
    "subset_dataset = TensorDataset(subset_inputs, subset_targets)\n",
    "subset_inputs1, subset_targets1 = dep.extract_ordered_subset_from_loader(test_loader1, Date_Position)\n",
    "subset_dataset1 = TensorDataset(subset_inputs1, subset_targets1)\n",
    "subset_inputs2, subset_targets2 = dep.extract_ordered_subset_from_loader(test_loader2, Date_Position)\n",
    "subset_dataset2 = TensorDataset(subset_inputs2, subset_targets2)\n",
    "subset_inputs3, subset_targets3 = dep.extract_ordered_subset_from_loader(test_loader3, Date_Position)\n",
    "subset_dataset3 = TensorDataset(subset_inputs3, subset_targets3)\n",
    "# Create a DataLoader with the subset dataset based on TIGG data\n",
    "subset_loader = DataLoader(subset_dataset, batch_size=1, shuffle=False)\n",
    "subset_loader1 = DataLoader(subset_dataset1, batch_size=1, shuffle=False)\n",
    "subset_loader2 = DataLoader(subset_dataset2, batch_size=1, shuffle=False)\n",
    "subset_loader3 = DataLoader(subset_dataset3, batch_size=1, shuffle=False)\n",
    "def ModelPredict(subset_loader, model):\n",
    "    Data = list() # For prediction\n",
    "    Target = list() # For Ground Truth\n",
    "    C = 0\n",
    "    for input, target in subset_loader:\n",
    "            input, target = input.to(device),target.to(device)\n",
    "            output = model(input)\n",
    "            T,P = target[0,0].cpu().detach().numpy(), output[0,0].cpu().detach().numpy()\n",
    "            if norm == 'znorm':\n",
    "                T,P = ut.iznorm(T, Gmean, Gstd), ut.iznorm(P, Gmean, Gstd)\n",
    "            if norm == 'norm':\n",
    "                T,P = ut.inorm(T, Gmean, Gstd), ut.inorm(P, Gmean, Gstd)\n",
    "            Data.append(P)\n",
    "            Target.append(T)\n",
    "    return np.array(Data), np.array(Target)\n",
    "ModelPrediction, Model_GT = ModelPredict(subset_loader, model)\n",
    "ModelPrediction1, _ = ModelPredict(subset_loader1, model1)\n",
    "ModelPrediction2, _ = ModelPredict(subset_loader2, model2)\n",
    "ModelPrediction3, _ = ModelPredict(subset_loader3, model3)\n",
    "#-------------------------------------------------------------------#\n",
    "TrainPred, Train_GT = ModelPredict(train_loader, model)\n",
    "TrainPred1, _ = ModelPredict(train_loader, model1)\n",
    "TrainPred2, _ = ModelPredict(train_loader, model2)\n",
    "TrainPred3, _ = ModelPredict(train_loader, model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af506b3-6eb3-4fbf-a6b6-677f884d7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid model: Linera regression: Y - UNET = W1*(NWP-UNET) + W2\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "Months = np.arange(0, 55)\n",
    "UNET_flat = ModelPrediction2[Months].reshape(-1)\n",
    "TIGG_flat = TIGG_Pred[Months].reshape(-1)\n",
    "GPM_flat = Model_GT[Months].reshape(-1)\n",
    "#-------------------------------Compute difference -------------------------------------#\n",
    "X_diff = TIGG_flat - UNET_flat \n",
    "Y_diff = GPM_flat - UNET_flat\n",
    "#------------------------------------------------------------------------------------------\n",
    "# Train the linear regression model\n",
    "regressor = LinearRegression(fit_intercept=False)\n",
    "regressor.fit(X_diff.reshape(-1, 1), Y_diff)\n",
    "# Get the learned weights and intercept\n",
    "W1, W2 = regressor.coef_[0], regressor.intercept_\n",
    "print(f\"W1 (weight): {W1}, W2 (intercept): {W2}\")\n",
    "#------------------------------------------------------------------------------------------\n",
    "ensemble_avg = np.zeros_like(ModelPrediction2)\n",
    "for i in range(ensemble_avg.shape[0]):  # Loop over images\n",
    "    for j in range(ensemble_avg.shape[1]):  # Loop over rows (pixels)\n",
    "        for k in range(ensemble_avg.shape[2]):  # Loop over columns (pixels)\n",
    "            dif = TIGG_Pred[i, j, k] - ModelPrediction2[i, j, k]\n",
    "            val = regressor.predict(dif.reshape(-1,1)) #Y - UNET\n",
    "            ensemble_avg[i, j, k] = val + ModelPrediction2[i, j, k]  #Y = pred + UNET \n",
    "print(\"Ensemble Matrix Shape:\", ensemble_avg.shape)\n",
    "#------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1b0216-0e3d-4f79-8687-c51b0c86b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual(Bar=False, save_path=None, **images):\n",
    "    \"\"\"Plot images in one row with a single color bar and optionally save the plot.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    # Find the common color range\n",
    "    vmin = min(image.min() for image in images.values())\n",
    "    vmax = max(image.max() for image in images.values())\n",
    "    # Generate random locations once\n",
    "    first_image = list(images.values())[0]\n",
    "    x_locs = np.random.randint(0, first_image.shape[1], 3)\n",
    "    y_locs = np.random.randint(0, first_image.shape[0], 3)\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        if '_' in name:\n",
    "            base_name, suffix = name.rsplit('_', 1)  # Split name and suffix\n",
    "            plt.title(f\"{base_name}$_{{{suffix}}}$\")\n",
    "        else:\n",
    "            plt.title(name)\n",
    "        img = plt.imshow(image, cmap='viridis', vmin=vmin, vmax=vmax)  # Use common color range\n",
    "    if Bar:\n",
    "        cbar = plt.colorbar(img, ax=plt.gca(), orientation='vertical', fraction=0.02, pad=0.04)\n",
    "        cbar.ax.tick_params(labelsize=15)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=1000)\n",
    "    plt.show()\n",
    "I = [49, 14, 34]\n",
    "for r in range(0,len(I)):\n",
    "    i = I[r]\n",
    "    PATH = './Result/SamplePredImage'+str(i)+'.png'\n",
    "    visual(Bar=True, save_path=None, GT=Model_GT[i], CLIM=Random_Prediction[i], NWP=TIGG_Pred[i],\n",
    "           UNET_18=ModelPrediction2[i], UNET_12=ModelPrediction3[i], HYB = ensemble_avg[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbaaf18-67f5-4c40-9b75-c2933a690bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_grid(image_dict_rows, save_path=None):\n",
    "    nrows = len(image_dict_rows)\n",
    "    ncols = max(len(row) for row in image_dict_rows)\n",
    "\n",
    "    # Find global vmin/vmax\n",
    "    vmin = min(image.min() for row in image_dict_rows for image in row.values())\n",
    "    vmax = max(image.max() for row in image_dict_rows for image in row.values())\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(3 * ncols, 3 * nrows),\n",
    "                             constrained_layout=False, squeeze=False)\n",
    "\n",
    "    for i, row in enumerate(image_dict_rows):\n",
    "        for j, (name, image) in enumerate(row.items()):\n",
    "            ax = axes[i, j]\n",
    "            im = ax.imshow(image, cmap='Blues', vmin=vmin, vmax=vmax)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "            # Add titles only on top row\n",
    "            if i == 0:\n",
    "                if '_' in name:\n",
    "                    base_name, suffix = name.rsplit('_', 1)\n",
    "                    ax.set_title(f\"{base_name}$_{{{suffix}}}$\", fontsize=12)\n",
    "                else:\n",
    "                    ax.set_title(name, fontsize=15)\n",
    "        # Hide unused axes in this row\n",
    "        for j in range(len(row), ncols):\n",
    "            axes[i, j].axis('off')\n",
    "\n",
    "    # Colorbar at bottom\n",
    "    cbar_ax = fig.add_axes([0.2, 0.05, 0.6, 0.02])  # [left, bottom, width, height]\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "    cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "    # Reduce spacing between images\n",
    "    plt.subplots_adjust(wspace=0.03, hspace=0.03, bottom=0.09)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=900)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "visual_grid([\n",
    "    {'GT': Model_GT[49], 'CLIM': Random_Prediction[49], 'NWP': TIGG_Pred[49], 'UNET_18': ModelPrediction2[49], 'UNET_12': ModelPrediction3[49], 'HYB': ensemble_avg[49]},\n",
    "    {'GT': Model_GT[14], 'CLIM': Random_Prediction[14], 'NWP': TIGG_Pred[14], 'UNET_18': ModelPrediction2[14], 'UNET_12': ModelPrediction3[14], 'HYB': ensemble_avg[14]},\n",
    "    {'GT': Model_GT[34], 'CLIM': Random_Prediction[34], 'NWP': TIGG_Pred[34], 'UNET_18': ModelPrediction2[34], 'UNET_12': ModelPrediction3[34], 'HYB': ensemble_avg[34]}],\n",
    "            save_path = None)#'./Result/SamplePredImage1.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ba5c1-570a-4e0c-a032-a1c58afae3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Evaluation. MAE (GroundTruth, Prediction)\n",
    "def mae_and_std(Target, Data):\n",
    "    mae_values = np.mean(np.abs(Data - Target), axis=0) #MAE per location across all samples\n",
    "    print(mae_values.shape, Target.shape)\n",
    "    avg_mae = np.mean(mae_values) #Mean of MAE\n",
    "    avg_std = np.std(mae_values) #STD of MAE\n",
    "    return avg_mae, avg_std\n",
    "    \n",
    "TIGG_MAE, random_MAE = mae_and_std(Model_GT, TIGG_Pred), mae_and_std(Random_GT, Random_Prediction)\n",
    "model_MAE, model_MAE1, model_MAE2, model_MAE3 = mae_and_std(Model_GT, ModelPrediction), mae_and_std(Model_GT, ModelPrediction1), mae_and_std(Model_GT, ModelPrediction2), mae_and_std(Model_GT, ModelPrediction3)\n",
    "ensemble_MAE = mae_and_std(Model_GT, ensemble_avg)\n",
    "# Print header\n",
    "print(\"\\n{:<25} {:<15} {:<15}\".format(\"Model\", \"MAE\", \"STD\"))\n",
    "print(\"=\" * 50)\n",
    "# Print results in a structured format\n",
    "print(\"{:<25} {:<15.6f} {:<15.6f}\".format(\"1. CLIM\", random_MAE[0], random_MAE[1]))\n",
    "print(\"{:<25} {:<15.6f} {:<15.6f}\".format(\"2. TIGG\", TIGG_MAE[0], TIGG_MAE[1]))\n",
    "print(\"{:<25} {:<15.6f} {:<15.6f}\".format(\"3. UNET_18\", model_MAE2[0], model_MAE2[1]))\n",
    "print(\"{:<25} {:<15.6f} {:<15.6f}\".format(\"4. UNET_12\", model_MAE3[0], model_MAE3[1]))\n",
    "print(\"{:<25} {:<15.6f} {:<15.6f}\".format(\"5. HYB\", ensemble_MAE[0], ensemble_MAE[1]))\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3152df2-3253-4b70-a040-50c8c2ccc636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the error matrix:\n",
    "M = TIGG_Pred - Model_GT\n",
    "#np.save(\"./Result/NWP_Samplewise_Error.npy\", M)\n",
    "M = Random_Prediction - Random_GT\n",
    "#np.save(\"./Result/CLIM_Samplewise_Error.npy\", M)\n",
    "M = ModelPrediction2 - Model_GT\n",
    "#np.save(\"./Result/UNET18_Samplewise_Error.npy\", M)\n",
    "M = ModelPrediction3 - Model_GT\n",
    "#np.save(\"./Result/UNET12_Samplewise_Error.npy\", M)\n",
    "M = ensemble_avg - Model_GT\n",
    "#np.save(\"./Result/HYB_Samplewise_Error.npy\", M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31b736-3f74-41d3-bd5d-32ebb0243716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparehyb_data(Ind_TIG_ens, ModelPrediction2, regressor):\n",
    "    dif = Ind_TIG_ens - ModelPrediction2[:, np.newaxis, :, :]\n",
    "    dif_flat = dif.reshape(-1, 1)\n",
    "    val_flat = regressor.predict(dif_flat)\n",
    "    val = val_flat.reshape(Ind_TIG_ens.shape)\n",
    "    ensemble_avg = val + ModelPrediction2[:, np.newaxis, :, :]\n",
    "    return ensemble_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acca01a-9d65-4f6b-978a-2ec1ce4c3b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRPS\n",
    "def compute_crps(x_values, y):\n",
    "    n = len(x_values)\n",
    "    x_values = sorted(x_values)  # Ensure the data is sorted\n",
    "    k = next((i for i in range(n-1) if x_values[i] <= y <= x_values[i+1]), n-1) # Find k such that x_k <= y <= x_k+1\n",
    "    crps = 0\n",
    "    #----------------- Sum for j < k --------------------------------------#\n",
    "    for j in range(k):\n",
    "        crps += (x_values[j+1] - x_values[j]) * ( (j+1)/n )**2\n",
    "    #------------------ Contribution from the interval [x_k, x_k+1]------------------#\n",
    "    if k < n - 1:  # Ensure x_k+1 exists\n",
    "        crps += ((k+1)/n)**2 * (y - x_values[k]) + ((k+1)/n - 1)**2 * (x_values[k+1] - y)    \n",
    "    #----------------- Sum for j > k --------------------------------------#\n",
    "    for j in range(k+1, n-1):\n",
    "        crps += (x_values[j+1] - x_values[j]) * ( (j+1)/n - 1 )**2\n",
    "    return crps\n",
    "def manual_crps(Ind_Random, Random_GT):\n",
    "    crps_clim = []\n",
    "    for i in range(len(Ind_Random)):\n",
    "        map = np.zeros((64, 64))\n",
    "        if np.mean(Random_GT[i])>0.0:\n",
    "            for j in range(Random_GT.shape[1]):\n",
    "                for k in range(Random_GT.shape[2]):\n",
    "                    x_values, y = np.array(Ind_Random[i])[:,j,k], Random_GT[i,j,k]\n",
    "                    map[j,k] = compute_crps(x_values, y)\n",
    "            crps_clim.append(map)\n",
    "            #print(f'The CRPS for the image is: {np.mean(map)} and std dev. is {np.std(map)}')\n",
    "    crps_clim = np.mean(np.array(crps_clim), axis=0)\n",
    "    #print(np.mean(crps_clim))\n",
    "    return crps_clim\n",
    "\n",
    "def compute_crps_per_pixel(i, j, train_preds, train_target, val_preds, val_target):\n",
    "    \"\"\" Compute CRPS for a single pixel (i, j). \"\"\"\n",
    "    idr_per_grid = idr(y=train_target[:, i, j], X=pd.DataFrame(train_preds[:, i, j]))\n",
    "    val_dist_pred = idr_per_grid.predict(pd.DataFrame(val_preds[:, i, j]))\n",
    "    return np.mean(val_dist_pred.crps(val_target[:, i, j]))\n",
    "\n",
    "def calculate_crps_idr_parallel(val_preds, val_target, train_preds, train_target):\n",
    "    grid_size = 64  # Assuming a 64x64 grid\n",
    "    crps_map = np.zeros((grid_size, grid_size))\n",
    "    results = Parallel(n_jobs=-1)(delayed(compute_crps_per_pixel)(i, j, train_preds, train_target, val_preds, val_target)\n",
    "                                  for i in range(grid_size) for j in range(grid_size))\n",
    "\n",
    "    crps_map = np.array(results).reshape(grid_size, grid_size)\n",
    "    return crps_map\n",
    "\n",
    "\n",
    "crps_clim = manual_crps(Ind_Random, Random_GT)\n",
    "crps_tigg = manual_crps(Ind_TIG_ens, GroundTruth)\n",
    "crps_model2 = calculate_crps_idr_parallel(ModelPrediction2, Model_GT, TrainPred2, Train_GT) #EASYUQ\n",
    "crps_model3 = calculate_crps_idr_parallel(ModelPrediction3, Model_GT, TrainPred3, Train_GT)\n",
    "crps_hyb = manual_crps(preparehyb_data(Ind_TIG_ens, ModelPrediction2, regressor), Model_GT)\n",
    "\n",
    "print(\"\\n{:<25} {:<15}\".format(\"Model\", \"CRPS Mean | STD\"))  # Print header\n",
    "print(\"=\" * 42)\n",
    "print(\"{:<25} {:<7.6f} | {:<7.6f}\".format(\"1. CLIM\", np.mean(crps_clim), np.std(crps_clim)))\n",
    "print(\"{:<25} {:<7.6f} | {:<7.6f}\".format(\"2. TIGG\", np.mean(crps_tigg), np.std(crps_tigg)))\n",
    "print(\"{:<25} {:<7.6f} | {:<7.6f}\".format(\"3. UNET_18\", np.mean(crps_model2), np.std(crps_model2)))\n",
    "print(\"{:<25} {:<7.6f} | {:<7.6f}\".format(\"4. UNET_12\", np.mean(crps_model3), np.std(crps_model3)))\n",
    "print(\"{:<25} {:<7.6f} | {:<7.6f}\".format(\"5. HYB\", np.mean(crps_hyb), np.std(crps_hyb)))\n",
    "print(\"=\" * 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a564d-b1ef-49bc-9763-db6e5d1bbaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Skill_score (ref, binary = True, **models):\n",
    "    n = len(models)\n",
    "    skill_scores = []\n",
    "    for name, pred in models.items():\n",
    "        skill = (ref - pred)/ref\n",
    "        if binary: skill = np.where(skill < 0, -1, np.where(skill > 0, 1, 0))\n",
    "        skill_scores.append(skill)\n",
    "    return skill_scores\n",
    "\n",
    "skills = Skill_score(crps_clim, binary=True, TIG=crps_tigg, UNET_18=crps_model2, UNET_12=crps_model3, HYB=crps_hyb)\n",
    "#visual(Bar=True, save_path=None, CLIM_crps = crps_clim, TIGG=crps_tigg, Model2=crps_model2, Model3=crps_model3)\n",
    "#visual(Bar=True, save_path=None, TIGG = skills[0], UNET_18 = skills[1], UNET_12 = skills[2])\n",
    "models = {\"CLIM\": np.expand_dims(crps_clim,axis=0),\"NWP\": np.expand_dims(crps_tigg, axis=0),\"UNET_18\": np.expand_dims(crps_model2, axis=0),\n",
    "          \"UNET_12\": np.expand_dims(crps_model3, axis=0),\"HYB\": np.expand_dims(crps_hyb, axis=0)}\n",
    "interact(lambda time_step: plot_data(time_step, cbar_label='CRPS score', save='./Result/CRPS.png'),\n",
    "         time_step=widgets.IntSlider(min=0, max=Model_GT.shape[0]-1, step=1, value=0));\n",
    "\n",
    "models = {\"NWP\": np.expand_dims(skills[0], axis=0), \"UNET_18\": np.expand_dims(skills[1], axis=0),\n",
    "          \"UNET_12\": np.expand_dims(skills[2], axis=0), \"HYB\": np.expand_dims(skills[3], axis=0)}\n",
    "interact(lambda time_step: plot_data(time_step, cbar_label='CRPS Skill', save='./Result/CRPS_skill.png'),\n",
    "         time_step=widgets.IntSlider(min=0, max=Model_GT.shape[0]-1, step=1, value=0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7a5d4c-273c-45ed-9fd1-c2ca11dd92d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin_edges = [-1, -0.1, 0, 0.1, 1] + list(np.linspace(1, 5, num=48))\n",
    "bin_edges = [-1, -0.1, 0, 0.1, 1]\n",
    "#bin_edges = [-1, 0, 1]\n",
    "# Target and Data should be numpy arrays of shape (N, W, H)\n",
    "hist1, bin_edges1 = dep.compute_histogram(Random_GT, Random_Prediction, bin_edges, Print=False, Hist=False)\n",
    "hist2, bin_edges2 = dep.compute_histogram(GroundTruth, TIGG_Pred, bin_edges, Print=False, Hist=False)\n",
    "hist3, bin_edges3 = dep.compute_histogram(Model_GT, ModelPrediction2, bin_edges, Print=False, Hist=False)\n",
    "hist4, bin_edges4 = dep.compute_histogram(Model_GT, ModelPrediction3, bin_edges, Print=False, Hist=False)\n",
    "hist5, bin_edges5 = dep.compute_histogram(Model_GT, ensemble_avg, bin_edges, Print=False, Hist=False)\n",
    "\n",
    "# Create column headers dynamically based on bin ranges\n",
    "header = [\"Model\"] + [f\"Range {i} ({bin_edges[i]} to {bin_edges[i+1]})\" for i in range(len(bin_edges)-1)]\n",
    "\n",
    "# Print header\n",
    "print(\"\\n\" + \"{:<20}\".format(header[0]) + \" \".join([\"{:<15}\".format(h) for h in header[1:]]))\n",
    "print(\"=\" * (20 + 15 * (len(bin_edges)-1)))\n",
    "# Print histogram values for each model\n",
    "print(\"{:<20} {}\".format(\"1. CLIM\", \" \".join(\"{:<15}\".format(h) for h in hist1)))\n",
    "print(\"{:<20} {}\".format(\"2. TIGG\", \" \".join(\"{:<15}\".format(h) for h in hist2)))\n",
    "print(\"{:<20} {}\".format(\"3. UNET_18\", \" \".join(\"{:<15}\".format(h) for h in hist3)))\n",
    "print(\"{:<20} {}\".format(\"4. UNET_12\", \" \".join(\"{:<15}\".format(h) for h in hist4)))\n",
    "print(\"{:<20} {}\".format(\"5. HYB\", \" \".join(\"{:<15}\".format(h) for h in hist5)))\n",
    "print(\"=\" * (20 + 15 * (len(bin_edges)-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb7dbd-5d02-44db-84cc-86ef900a3b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Evaluation: TS/CSI, FPR, POD (GroundTruth, Prediction, threshold)\n",
    "#1/100 (0.01) of an inch of rain â€“ The first measurable amount of rainfall reported by The National Weather Service\n",
    "def score(ground_truth, forecast, threshold):\n",
    "    TP, FP, FN, TN = [], [], [], []\n",
    "    for sample in range(len(ground_truth)):\n",
    "        gt_mask = ground_truth[sample] > threshold\n",
    "        forecast_mask = forecast[sample] > threshold\n",
    "        TP.append(np.logical_and(forecast_mask, gt_mask))\n",
    "        FP.append(np.logical_and(forecast_mask, np.logical_not(gt_mask)))\n",
    "        FN.append(np.logical_and(np.logical_not(forecast_mask), gt_mask))\n",
    "        TN.append(np.logical_and(np.logical_not(forecast_mask), np.logical_not(gt_mask)))\n",
    "    TP, FP, FN, TN = map(lambda x: np.array(x).astype(int), [TP, FP, FN, TN])\n",
    "    TP_sum, FP_sum, FN_sum, TN_sum = np.sum(TP, axis=0), np.sum(FP, axis=0), np.sum(FN, axis=0), np.sum(TN, axis=0)\n",
    "    precision = np.divide(TP_sum, TP_sum + FP_sum, out=np.zeros_like(TP_sum, dtype=float), where=(TP_sum + FP_sum)!=0)\n",
    "    CSI = np.divide(TP_sum, TP_sum + FP_sum + FN_sum, out=np.zeros_like(TP_sum, dtype=float), where=(TP_sum + FP_sum + FN_sum)!=0)\n",
    "    recall = np.divide(TP_sum, TP_sum + FN_sum, out=np.zeros_like(TP_sum, dtype=float), where=(TP_sum + FN_sum)!=0)\n",
    "    accuracy = np.divide(TP_sum + TN_sum, TP_sum + FP_sum + FN_sum + TN_sum, out=np.zeros_like(TP_sum, dtype=float), \n",
    "                         where=(TP_sum + FP_sum + FN_sum + TN_sum)!=0)\n",
    "    return precision, recall, CSI, accuracy\n",
    "\n",
    "th = [0.5, 10]\n",
    "for t in th:\n",
    "    print('\\n-------------------------The Results based on the threshold=', t, '---------------------------')\n",
    "    thresholds = [t]\n",
    "    print(\"\\n{:<20} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\"Model\", \"CSI\", \"Precision\", \"Recall\", \"F1\", \"P_loc\", \"R_loc\", \"F1_loc\"))\n",
    "    print(\"=\" * 95)\n",
    "    \n",
    "    Csi, Precision, Recall, F1, ACC = dep.score_calculate(Random_GT, Random_Prediction, thresholds)\n",
    "    Random_P, Random_R, Random_C, Random_ACC = score(Random_GT,Random_Prediction, t)\n",
    "    print(\"{:<20} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\"\n",
    "          .format(\"1. CLIM\", Csi, Precision, Recall, F1, np.mean(Random_P), np.mean(Random_R), np.mean(Random_C)))\n",
    "    print(ACC)\n",
    "    \n",
    "    Csi, Precision, Recall, F1, ACC = dep.score_calculate(GroundTruth, TIGG_Pred, thresholds)\n",
    "    TIGG_P, TIGG_R, TIGG_C, TIGG_ACC = score(GroundTruth,TIGG_Pred, t)\n",
    "    print(\"{:<20} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\"\n",
    "          .format(\"2. TIGG\",Csi, Precision, Recall, F1, np.mean(TIGG_P), np.mean(TIGG_R), np.mean(TIGG_C)))\n",
    "    print(ACC)\n",
    "    \n",
    "    Csi, Precision, Recall, F1, ACC = dep.score_calculate(Model_GT, ModelPrediction2, thresholds)\n",
    "    precision2, recall2, C2, ACC2 = score(Model_GT, ModelPrediction2, t)\n",
    "    print(\"{:<20} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\"\n",
    "          .format(\"3. UNET_18\", Csi, Precision, Recall, F1, np.mean(precision2), np.mean(recall2), np.mean(C2)))\n",
    "    print(ACC)\n",
    "    \n",
    "    Csi, Precision, Recall, F1, ACC = dep.score_calculate(Model_GT, ModelPrediction3, thresholds)\n",
    "    precision3, recall3, C3, ACC3 = score(Model_GT, ModelPrediction3, t)\n",
    "    print(\"{:<20} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\"\n",
    "          .format(\"4. UNET_12\", Csi, Precision, Recall, F1, np.mean(precision3), np.mean(recall3), np.mean(C3)))\n",
    "    print(ACC)\n",
    "    \n",
    "    Csi, Precision, Recall, F1, ACC = dep.score_calculate(Model_GT, ensemble_avg, thresholds)\n",
    "    Ensemble_P, Ensemble_R, Ensemble_C, Ensemble_ACC = score(Model_GT, ensemble_avg, t)\n",
    "    print(\"{:<20} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\"\n",
    "          .format(\"5. HYB\", Csi, Precision, Recall, F1, np.mean(Ensemble_P), np.mean(Ensemble_R), np.mean(Ensemble_C)))\n",
    "    print(ACC)\n",
    "    print(\"=\" * 95)\n",
    "    \n",
    "    models = {\"CLIM\": np.expand_dims(Random_P, axis=0), \"NWP\": np.expand_dims(TIGG_P,axis=0), \"UNET_18\": np.expand_dims(precision2, axis=0),\n",
    "              \"UNET_12\": np.expand_dims(precision3, axis=0), \"HYB\": np.expand_dims(Ensemble_P, axis=0)}\n",
    "    interact(lambda time_step: plot_data(time_step, cbar_label='Precision', save='./Result/Precision_'+str(t)+'.png'),\n",
    "             time_step=widgets.IntSlider(min=0, max=precision2.shape[0]-1, step=1, value=0));\n",
    "    models = {\"CLIM\": np.expand_dims(Random_R, axis=0), \"NWP\": np.expand_dims(TIGG_R,axis=0), \"UNET_18\": np.expand_dims(recall2, axis=0),\n",
    "              \"UNET_12\": np.expand_dims(recall3, axis=0), \"HYB\": np.expand_dims(Ensemble_R, axis=0)}\n",
    "    interact(lambda time_step: plot_data(time_step, cbar_label='Recall', save='./Result/Recall_'+str(t)+'.png'),\n",
    "             time_step=widgets.IntSlider(min=0, max=recall2.shape[0]-1, step=1, value=0));\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa66a78b-1cbc-47ff-b5c0-f92adb3a8677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proportion calculation\n",
    "def proportion(ground_truth, threshold):\n",
    "    total_pixels = np.prod(ground_truth.shape)\n",
    "    nonzero_pixels = np.sum(ground_truth > 0)\n",
    "    hits = np.sum(ground_truth > threshold)\n",
    "    proportion = hits / total_pixels\n",
    "    conditional = hits / nonzero_pixels if nonzero_pixels > 0 else 0.0\n",
    "    print(f'Proportion: {proportion}| Conditional: {conditional}')\n",
    "for i in [0.0, 0.5, 10]:\n",
    "    proportion(GroundTruth, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c221be-b85d-4f41-ab2f-c93175a6d23e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
